# -*- coding: utf-8 -*-
"""「Colab 6 - Transfer Learning and Fine Tunning.ipynb」的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uq_pPZMJ4MTAFdcgSC67ZL2ypfGZUACH

![alt text](https://live.staticflickr.com/4544/38228876666_3782386ca7_b.jpg)

## Stage 1: Install dependencies and setting up GPU environment
"""

!pip install tensorflow-gpu==2.0.0.alpha0

!pip install tqdm

"""### Downloading the Dogs vs Cats dataset """

!wget --no-check-certificate 
    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \
    -O ./cats_and_dogs_filtered.zip

"""## Stage 2: Dataset preprocessing

### Import project dependencies
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import zipfile
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

from tqdm import tqdm_notebook
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# %matplotlib inline
tf.__version__

"""### Unzipping the Dogs vs Cats dataset"""

dataset_path = "./cats_and_dogs_filtered.zip"

zip_object = zipfile.ZipFile(file=dataset_path, mode="r")

zip_object.extractall("./")

zip_object.close()

"""### Seting up dataset paths"""

dataset_path_new = "./cats_and_dogs_filtered/"

train_dir = os.path.join(dataset_path_new, "train")
validation_dir = os.path.join(dataset_path_new, "validation")

"""## Building the model

### Loading the pre-trained model (MobileNetV2)
"""

IMG_SHAPE = (128, 128, 3)

base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights="imagenet")

base_model.summary()

"""### Freezing the base model"""

base_model.trainable = False

"""### Defining the custom head for our network"""

base_model.output

global_average_layer = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)

global_average_layer

prediction_layer = tf.keras.layers.Dense(units=1, activation='sigmoid')(global_average_layer)

"""### Defining the model"""

model = tf.keras.models.Model(inputs=base_model.input, outputs=prediction_layer)

model.summary()

"""### Compiling the model"""

model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001), loss="binary_crossentropy", metrics=["accuracy"])

"""### Creating Data Generators

Resizing images

    Big pre-trained architecture support only certain input sizes.

For example: MobileNet (architecture that we use) supports: (96, 96), (128, 128), (160, 160), (192, 192), (224, 224).
"""

data_gen_train = ImageDataGenerator(rescale=1/255.)
data_gen_valid = ImageDataGenerator(rescale=1/255.)

train_generator = data_gen_train.flow_from_directory(train_dir, target_size=(128,128), batch_size=128, class_mode="binary")

valid_generator = data_gen_valid.flow_from_directory(validation_dir, target_size=(128,128), batch_size=128, class_mode="binary")

"""### Training the model"""

model.fit_generator(train_generator, epochs=5, validation_data=valid_generator)

"""### Transfer learning model evaluation"""

valid_loss, valid_accuracy = model.evaluate_generator(valid_generator)

print("Accuracy after transfer learning: {}".format(valid_accuracy))

"""## Fine tuning


There are a few pointers:

- DO NOT use Fine tuning on the whole network; only a few top layers are enough. In most cases, they are more specialized. The goal of the Fine-tuning is to adopt that specific part of the network for our custom (new) dataset.
- Start with the fine tunning AFTER you have finished with transfer learning step. If we try to perform Fine tuning immediately, gradients will be much different between our custom head layer and a few unfrozen layers from the base model.

### Un-freeze a few top layers from the model
"""

base_model.trainable = True

print("Number of layersin the base model: {}".format(len(base_model.layers)))

fine_tune_at = 100

for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

"""### Compiling the model for fine-tuning"""

model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.0001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

"""### Fine tuning"""

model.fit_generator(train_generator,  
                    epochs=5, 
                    validation_data=valid_generator)

"""### Evaluating the fine tuned model"""

valid_loss, valid_accuracy = model.evaluate_generator(valid_generator)

print("Validation accuracy after fine tuning: {}".format(valid_accuracy))

